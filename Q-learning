import numpy as np
import pandas as pd
import os
import logging
import itertools
import matplotlib.pyplot as plt
import random

GRID_SIZE = 10
N_STATES = GRID_SIZE ** 2
EPSILON = 1.0  # Initial exploration rate
EPSILON_DECAY = 0.99  # Decay rate for exploration rate
ALPHA = 0.001  # Learning rate
GAMMA = 0.95  # Discount factor
MAX_ENERGY = 10000
RESOURCE_VALUE = 100
SIZE = 2.5  # micro b
RATE = 50  # micro b
STEP_SIZE = 1

class Robot:
    def __init__(self, name, pos_x, pos_y, actions, workload, resources):
        self.name = name
        self.pos_x = pos_x
        self.pos_y = pos_y
        self.actions = actions
        self.workload = workload  # lambda workload value λ: 0 --> 10
        self.resources = resources  # robot resources: res 0 --> 100: equivalent to MIPS in this case
        self.energy = MAX_ENERGY  # 10K JOULES ENERGY FIXED VALUE

    def reset(self):
        self.pos_x = random.randint(0, GRID_SIZE)
        self.pos_y = random.randint(0, GRID_SIZE)
        self.workload = random.randint(1, 10)  # lambda workload value λ: 0 --> 10
        self.resources = random.randint(1, 100)  # robot resources: res 0 --> 100: equivalent to MIPS in this case
        return self

    def get_state(self):
        position = (self.pos_x, self.pos_y)
        remaining_energy = self.energy
        available_resource = self.resources
        workload = self.workload
        return [position, remaining_energy, available_resource, workload]

    def get_action(self, direction, target_robot):
        self.offload(target_robot)
        if direction == 'u' and self.pos_y > 0:
            self.pos_y -= 1
        elif direction == 'l' and self.pos_x > 0:
            self.pos_x -= 1
        elif direction == 'r' and self.pos_x < GRID_SIZE:
            self.pos_x += 1
        elif direction == 'd' and self.pos_y < GRID_SIZE:
            self.pos_y += 1
        else:
            print("keep same position")
            return None

    def offload(self, target_robot):
        if self.workload > 0 and self.resources > 5:
            self.workload -= 1
            self.resources -= 5
            target_robot.resources += 5
            target_robot.workload += 1
        return None

    def calculate_delay(self):
        processing_delay = self.workload / self.resources
        transmission_delay = SIZE / RATE
        return processing_delay + transmission_delay

    def calculate_energy_consumption(self):
        return self.workload / self.energy

    def reward(self):
        return 1 / self.calculate_delay()

def st(robots):
    return [robot.get_state() for robot in robots]

def state_index(robots, Q_table):
    state = st(robots)
    state_index = None
    for index, s in enumerate(Q_table):
        if np.array_equal(state, s):
            state_index = index
            break
    return state_index

def action_index(actions, robot_actions):
    action_index = None
    for index, a in enumerate(robot_actions):
        if np.array_equal(actions, a):
            action_index = index
            break
    return action_index

def initialize_Q_table(num_states, num_actions, num_agents):
    Q_table = np.zeros((num_states, num_actions))
    return Q_table

def main(epsilon):
    num_agents = 3
    num_actions = 5  # ['left', 'right', 'up', 'down', 'offload']
    num_states = N_STATES
    num_episodes = 100  # Increased number of episodes
    accreward = []

    Q_table = initialize_Q_table(num_states, num_actions ** num_agents, num_agents)

    f = open("results.txt", "w+")

    maxstep = 10  # Increased number of steps per episode
    for episode in range(num_episodes):
        f.write(f'Starting episode {episode + 1}\n')
        episode_rewards = []
        robots = [
            Robot("Robot 1", 0, 0, ['l', 'r', 'u', 'd', 'o'], 0, 0),
            Robot("Robot 2", 0, 0, ['l', 'r', 'u', 'd', 'o'], 0, 0),
            Robot("Robot 3", 0, 0, ['l', 'r', 'u', 'd', 'o'], 0, 0)
        ]

        for robot in robots:
            robot.reset()

        for i in range(maxstep):
            totreward = 0
            current_state = state_index(robots, Q_table)
            if current_state is None:
                current_state = len(Q_table)
                Q_table = np.vstack((Q_table, np.zeros(num_actions ** num_agents)))
            for agent, robot in enumerate(robots):
                if np.random.rand() < epsilon:
                    action = np.random.choice(robot.actions)
                else:
                    action = robot.actions[np.argmax(Q_table[current_state])]
                target_workload_robot = random.choice([r for r in robots if r != robot])
                robot.get_action(action, target_robot=target_workload_robot)
                new_state = state_index(robots, Q_table)
                if new_state is None:
                    new_state = len(Q_table)
                    Q_table = np.vstack((Q_table, np.zeros(num_actions ** num_agents)))

                reward = robot.reward()
                totreward += reward

                action_ind = action_index(action, robot.actions)
                Q_table[current_state][action_ind] = (1 - ALPHA) * Q_table[current_state][action_ind] + ALPHA * (
                        reward + GAMMA * np.max(Q_table[new_state]))

            episode_rewards.append(totreward)
            f.write(
                f'Episode {episode + 1}, Robot {agent + 1}, Action: {action}, Reward: {reward}, New State: {new_state}\n')

        epsilon *= EPSILON_DECAY  # Decay exploration rate
        accreward.append(sum(episode_rewards))  # sum of all the reward within an episode

    fig, ax = plt.subplots()
    plt.plot(range(1, num_episodes + 1), accreward)
    ax.legend()
    plt.show()
    f.close()

    return [robot.reward() for robot in robots]

if __name__ == "__main__":
    epsilon = EPSILON  # Initial exploration rate
    main(epsilon)
